# awesome-multi-modality [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
A list of awesome multi-modality resources like papers and codes, inspired by [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision).

## Table of Contents

- [Papers](#papers)

## Papers
- (cm-SSFT) [Cross-modality Person re-identification with Shared-Specific Feature Transfer](https://openaccess.thecvf.com/content_CVPR_2020/papers/Lu_Cross-Modality_Person_Re-Identification_With_Shared-Specific_Feature_Transfer_CVPR_2020_paper.pdf), Lu, Yan, et al., CVPR, 2020
  
- (CLIP) [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf), Radford, Alec, et al., PMLR, 2021 | [github](https://github.com/OpenAI/CLIP) 

- (ALBEF) [Align before Fuse: Vision and Language
Representation Learning with Momentum Distillation](https://arxiv.org/pdf/2107.07651.pdf), Li, Junnan, et al., NeurIPS, 2021 | [github](https://github.com/salesforce/ALBEF/)

- (MBT) [Attention Bottlenecks for Multimodal Fusion](https://arxiv.org/pdf/2107.00135.pdf), Nagrani, Arsha, et al., NeurIPS, 2021 | [github](https://github.com/google-research/scenic/tree/main/scenic/projects/mbt)

- (VLMO) [VLMO: Unified Vision-Language Pre-Training with
Mixture-of-Modality-Experts](https://arxiv.org/pdf/2111.02358.pdf), Bao, Hangbo, et al., NeurIPS, 2022 | [github](https://github.com/microsoft/unilm/tree/master/vlmo)

- (CoCa) [CoCa: Contrastive Captioners are Image-Text
Foundation Models](https://arxiv.org/pdf/2107.00135.pdf), Yu, Jiahui, et al., arXiv, 2022 |

- (BEITv3) [Image as a Foreign Language: BEIT Pretraining for
All Vision and Vision-Language Tasks](https://arxiv.org/pdf/2208.10442.pdf), Wang, Wenhui, et al., arXiv, 2022 | [github](https://github.com/microsoft/unilm/tree/master/beit3)

- (BLIP) [BLIP: Bootstrapping Language-Image Pre-training for
Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf), Li, Junnan, et al., PMLR, 2022 | [github](https://github.com/salesforce/BLIP)

- (Flamingo) [Flamingo: a Visual Language Model
for Few-Shot Learning](https://arxiv.org/pdf/2204.14198.pdf), Alayrac, Jean-Baptiste, et al., NeurIPS, 2022 | [github](https://github.com/salesforce/BLIP)


